# ml-introduction
Feature Engineering

[ ] App to generate your features and labels

[ ] Add all features you can think of

Prepare Problem:

[ ] Load Libraries

[ ] Load Dataset

Summarize Data

[ ] Data Exploration: Descriptive Statistics
     [ ] describe(): statistics
     [ ] class distribution
     [ ] correlations
     [ ] skew
     [ ] data types

[ ] Data Exploration: Visualization
     [ ] histogram
     [ ] density plot
     [ ] box whisker plot
     [ ] correlation matrix plot
     [ ] scatter plot matrix

Prepare Data

[ ] Data Cleaning

[ ] Data Pre-Processing

[ ] Rescale

[ ] Standardize

[ ] Normalize

[ ] Binarize

[ ] Feature Selection
     [ ] Univariate selection
     [ ] Recursive Feature Elimination
     [ ] Principal Component Analysis
     [ ] Feature Importance

[ ] Data Transform

Evaluate Algorithms: Data -> Algorithms -> Performance Metrics

[ ] Split-out validation dataset
Resampling

[ ] Train-Test Data split

[ ] k-fold cross validation

[ ] Loocv

[ ] Repeated Randon train-test splits

[ ] Test options and evaluation metric
Algorithm Performance Metrics

[ ] Classification Metrics
     [ ] Classification Accuracy
     [ ] Logarithmic Loss
     [ ] Area under ROC curve
     [ ] Confusion Matrix
     [ ] Classification Report: precision , recall, F1 score, support

[ ] Regression Metrics
     [ ] Mean Absolute Error
     [ ] Mean Squared Error
     [ ] R^2

[ ] Algorithm Performance: Spot-Check Algorithms

[ ] Classification

[ ] Linear Algorithms

[ ] Logistic Regression

[ ] Linear Discriminant Analysis

[ ] Non-Linear Algorithms

[ ] kNN

[ ] Naive Bayes

[ ] CART

[ ] SVM (SVC)

[ ] Regression

[ ] Linear Algorithms

[ ] Linear Regression

[ ] Ridge Regression

[ ] LASSO Linear Regression

[ ] ElasticNet Regression

[ ] Non-Linear Algorithms

[ ] kNN

[ ] CART

[ ] SVM (SVR)

[ ] Compare Algorithms
     [ ] Test harness to compare multiple algorithms efficiently on a single dataset
     [ ] Comparison visualization

[ ] Preventing Data Leakage using Pipelines

[ ] Data Preparation and Modeling Pipeling

[ ] Feature Extractions and Modeling Pipeling

Improve Accuracy

[ ] Algorithm Tuning: Hyperparameter Optimization

[ ] Grid Search Parameter Tuning

[ ] Random Search Parameter Tuning

[ ] Ensembles

[ ] Bagging

[ ] Bagged Decision Trees: Best for high variance algorithms

[ ] Random Forests

[ ] Extra Trees

[ ] Boosting

[ ] AdaBoost

[ ] Stochastic Gradient Boosting (Gradient Boosting Machines - GBM)

[ ] Voting

Finalize Model

[ ] Predictions on validation dataset

[ ] Create standalone model on entire training dataset

[ ] Save model for later use

[ ] pickle

[ ] joblib
